---
title: "RR Cluster Metrics"
output: html_notebook
---

## Setup and Configuration
```{r setup, message=FALSE}
# packages
library(aws.s3)
library(arrow)
library(duckdb)
library(dplyr)

# initialize DuckDB connection
con <- dbConnect(duckdb::duckdb())
```

##  OpenCitation
Create `.parquet` archive of doi-author_id-lastname pairings. This is done via a duckDB database built from the OpenCitations dataset saved as `.csv` files (50Gb). Dataset downloaded [from here](https://opencitations.net/download). The resulting parquet file is 5.2Gb. This code has been run outside of the notebook and is not executed here. Update filepath before executing.
```{r opencite, eval=FALSE}
# Create arrow dataset from OpenCitation .csv
openalex <- open_dataset(
  "<path-to-data>/data/csv_openalex",
  format = "csv"
)

# register arrow dataset as duckDB table
to_duckdb(openalex, con, "openalex")

# get doi-author_id pairs; save to .parquet
dbExecute(
  con,
  "COPY (
  SELECT
    doi,
    REGEXP_EXTRACT(pair.unnest, '^([^,]+)') AS lastname,
    REGEXP_EXTRACT(pair.unnest, 'omid:ra/\\d+') AS author_id
  FROM (
    SELECT
      REGEXP_EXTRACT(id, 'doi:[^ ]+') AS doi,
      REGEXP_EXTRACT_ALL(author, '[^;\\[]+\\[omid:ra/\\d+\\]') AS pairs
    FROM openalex
  ) t,
  UNNEST(pairs) AS pair
) TO 'doi_author_pairs.parquet' (FORMAT 'parquet');"
)
```

Examine first few rows of OpenCitations data.
```{r}
open_dataset("doi_author_pairs.parquet") |>
  head(500) |>
  collect()
```

## Top 75 Research Institutes
Prepare the dataframe for the researchers from the Top 75 Research Institutes
```{r}
# get list of prepared files
my_list <- list.files(
  "../../top_institutes_data_extraction/output",
  pattern = ".csv",
  full.names = TRUE
)

# combine into one
top_institutes <- purrr::map_df(my_list, read.csv)

top_institutes <- top_institutes |>
  filter(doi != "") |>
  select(doi, Lastname, uuid) |>
  mutate(doi = paste0("doi:", doi)) |>
  rename(
    "lastname" = Lastname,
    "author_id" = uuid
  )

arrow::write_parquet(
  top_institutes,
  "top_institutes.parquet"
)
```

## Ringgold | Researchers
Create `.parquet` archive of doi-author_id-lastname pairings. This is done via a duckDB database built from the `contribution` edges in SAND_DS. The resulting file is 112Mb. This code has been run outside of the notebook and is not executed here.
```{r, eval=FALSE}
# Create arrow dataset from edges and nodes parquet
# contribution edge
contribution <- open_dataset("s3://com-copyright-agpipeline-sandds/data/internal/parquet/r4r/phyeng/graph/edges/CONTRIBUTION/")
duckdb_register_arrow(con, "contribution", contribution)

# author node
da <- open_dataset("s3://com-copyright-agpipeline-sandds/data/internal/parquet/r4r/phyeng/graph/nodes/distinctauthor")
duckdb_register_arrow(con, "distinctAuthor", da)

# Get doi-author_id pairs; save to .parquet
dbExecute(
  con,
  "COPY (
  SELECT
    ct.target as doi,
    ct.source as author_id,
    da.lastname as lastname
  from contribution ct
  join distinctAuthor da on ct.source = da.id
  ) TO 'r4r.parquet' (FORMAT 'parquet');"
)
```

Examine first few rows of R|R data.
```{r}
open_dataset("r4r.parquet") |>
  head(50) |>
  collect()
```

## Calculate Metrics
### Dataset
Prepare filtered dataset of doi-author_id pairings using only DOIs found in Ringgold|Researcher data. The lastname is also used in the join to remove authors found on one dataset's article metadata that is not found in the other. The dataset is in the form of the doi to the true author_id (from Open Citations) to the predicted author_id (from RR).
```{r filtered_df, include=FALSE}
# Create arrow dataset for RR data
rr <- open_dataset("r4r.parquet")
# register r4r data in duckDB
to_duckdb(rr, con, "r4r") 

# # Create arrow dataset for OpenCitations data
# oc <- open_dataset("doi_author_pairs.parquet")
# to_duckdb(oc, con, "opencite") # register open citations data in duckDB

# Create arrow dataset for Top Institutes data
ti <- open_dataset("top_institutes.parquet")
to_duckdb(ti, con, "top_inst") # register open citations data in duckDB

# # Create filtered dataframe of doi to true author_id
# df <- dbGetQuery(
#   con,
#   "SELECT
#     r4r.doi,
#     r4r.author_id AS author_id_pred,
#     opencite.author_id AS author_id_true
#   FROM r4r
#   JOIN opencite ON r4r.doi = opencite.doi AND r4r.lastname = opencite.lastname"
# )

# Create filtered dataframe of doi to true author_id
df <- dbGetQuery(
  con,
  "SELECT
    r4r.doi,
    r4r.author_id AS author_id_pred,
    top_inst.author_id AS author_id_true
  FROM r4r
  JOIN top_inst ON r4r.doi = top_inst.doi AND r4r.lastname = top_inst.lastname"
)
```

Examine a few rows of the resulting dataframe
```{r}
head(df, 100)
```

### AAP
Create function to calculate AAP.

$$
\text{AAP} = \frac{1}{N} \sum_{j=1}^{t} \sum_{i=1}^{e} \frac{n_{ij}^2}{n_{j}}
$$

```{r aap}
calculate_aap <- function(df) {
  # Total number of records
  N <- nrow(df)

  # Compute n_ij = count of records in each true/predicted cluster pair
  n_ij <- df %>%
    count(author_id_true, author_id_pred, name = "n_ij")

  # Compute n_j = size of each true cluster
  n_j <- df %>%
    count(author_id_true, name = "n_j")

  # Join to associate n_j with each n_ij
  n_ij <- n_ij %>%
    left_join(n_j, by = "author_id_true")

  # Apply the formula
  aap <- sum((n_ij$n_ij^2) / n_ij$n_j) / N

  return(aap)
}
```

Apply to dataframe
```{r}
df |> calculate_aap()
```

### ACP
Create function to calculate ACP

$$
\text{ACP} = \frac{1}{N} \sum_{j=1}^{t} \sum_{i=1}^{e} \frac{n_{ij}^2}{n_i}
$$

```{r acp}
calculate_acp <- function(df) {
  N <- nrow(df)

  # Count n_ij: number of items in predicted cluster i and true cluster j
  n_ij <- df %>%
    count(author_id_pred, author_id_true, name = "n_ij")

  # Count n_i: total number of items in predicted cluster i
  n_i <- df %>%
    count(author_id_pred, name = "n_i")

  # Merge and compute ACP terms
  acp_df <- n_ij %>%
    left_join(n_i, by = "author_id_pred") %>%
    mutate(term = (n_ij^2) / n_i)

  # Final ACP calculation
  acp <- sum(acp_df$term) / N
  return(acp)
}
```

Calculate ACP for dataframe
```{r}
df |> calculate_acp()
```

